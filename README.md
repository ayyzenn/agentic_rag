# ğŸ§  Agentic RAG System with Gemini & ChromaDB

A sophisticated **Agentic Retrieval-Augmented Generation (RAG)** system featuring intelligent routing, advanced retrieval techniques, and autonomous decision-making. This system uses a 3-agent architecture to provide optimal answers for both simple and complex queries.

## ğŸŒŸ Features

- **ğŸ¤– 3-Agent Architecture**: Router, Basic Generator, and Advanced Generator agents work together intelligently
- **ğŸ§­ Intelligent Routing**: Automatically determines the best retrieval strategy for each query
- **ğŸš€ Advanced RAG Techniques**: 
  - Query Decomposition (breaks complex queries into sub-queries)
  - HyDE (Hypothetical Document Embeddings)
  - Multi-Query Retrieval (generates query variations)
- **âš¡ Fast & Efficient**: Uses ChromaDB for semantic search and Gemini Flash for generation
- **ğŸ¯ Adaptive**: Automatically switches from basic to advanced techniques when needed
- **ğŸ“Š Quality Evaluation**: Built-in answer quality assessment and confidence scoring

## ğŸ—ï¸ Architecture

```
User Query
    â†“
Router Agent (evaluates query complexity & routes)
    â†“
Basic Generator Agent (fast, simple retrieval)
    â†“
Router Agent (evaluates answer quality)
    â†“ (if insufficient)
Advanced Generator Agent
    â”œâ”€â”€ Query Decomposition
    â”œâ”€â”€ HyDE (Hypothetical Document Embeddings)
    â””â”€â”€ Multi-Query Retrieval
    â†“
Final Answer
```

### Agent Overview

1. **Router Agent**: Analyzes queries, evaluates answer quality, and routes to appropriate generator
2. **Basic Generator Agent**: Fast, straightforward RAG for simple queries using standard retrieval
3. **Advanced Generator Agent**: Employs advanced techniques for complex queries requiring deeper analysis

## ğŸ“‹ Prerequisites

- **Python 3.8+**
- **Google Gemini API Key** ([Get one here](https://aistudio.google.com/app/apikey))
- **4GB+ RAM** (for embeddings and model processing)

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd rag
```

### 2. Set Up Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Gemini API Key

Create a `.env` file in the project root:

```bash
echo "GEMINI_API_KEY=your_actual_api_key_here" > .env
```

Or manually create `.env` and add:
```
GEMINI_API_KEY=your_actual_api_key_here
```

### 5. Run the Agentic RAG System

```bash
python agentic_rag.py
```

## ğŸ“ Project Structure

```
rag/
â”œâ”€â”€ agentic_rag.py          # Main orchestrator and CLI
â”œâ”€â”€ config.py               # Configuration settings
â”œâ”€â”€ vector_store.py         # ChromaDB vector store wrapper
â”œâ”€â”€ preprocess.py           # Document preprocessing with metadata
â”œâ”€â”€ list_models.py          # Utility to list available Gemini models
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py       # Base agent class with Gemini integration
â”‚   â”œâ”€â”€ router_agent.py     # Router agent for query routing
â”‚   â”œâ”€â”€ basic_generator.py  # Basic RAG generator
â”‚   â””â”€â”€ advanced_generator.py # Advanced RAG with multiple techniques
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ prompt_templates.py  # Prompt templates for all agents
â”‚   â””â”€â”€ evaluator.py       # Answer quality evaluation
â”œâ”€â”€ docs/                   # Knowledge base documents
â”‚   â”œâ”€â”€ about_me.txt
â”‚   â”œâ”€â”€ education.txt
â”‚   â”œâ”€â”€ finance.txt
â”‚   â””â”€â”€ healthcare.txt
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ LICENSE                # MIT License
â”œâ”€â”€ SETUP.md               # Detailed setup guide
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Usage

### Running the System

```bash
python agentic_rag.py
```

You'll be prompted to select an output mode:
- **silent**: Shows only the final answer
- **verbose**: Shows routing decision and which agent was used (default)
- **debug**: Shows all steps, evaluations, and detailed reasoning

### Example Queries

**Simple Query (handled by Basic Generator):**
```
Ask your question: What is AI in healthcare?
```

**Complex Query (handled by Advanced Generator):**
```
Ask your question: Compare and contrast AI applications in healthcare versus finance, 
including specific use cases, benefits, and challenges for each domain.
```

### Utility Scripts

**List Available Gemini Models:**
```bash
python list_models.py
```

This helps you identify which Gemini models are available with your API key.

## ğŸ§  Advanced RAG Techniques

### 1. Query Decomposition
Automatically breaks complex, multi-part queries into simpler sub-queries, retrieves relevant documents for each, and synthesizes a comprehensive answer.

**Example:**
- Original: "Compare AI in healthcare vs finance"
- Decomposed: 
  - "AI applications in healthcare"
  - "AI applications in finance"
  - Synthesized comparison

### 2. HyDE (Hypothetical Document Embeddings)
Generates a hypothetical "ideal answer" first, then uses its embedding to find similar real documents for more focused retrieval.

**Process:**
1. Generate hypothetical answer using LLM
2. Embed the hypothetical answer
3. Search for documents similar to the hypothetical answer
4. Generate grounded answer from real documents

### 3. Multi-Query Retrieval
Creates multiple phrasings of the query to capture different perspectives and improve retrieval coverage.

**Example:**
- Original: "How is AI used in medicine?"
- Variations:
  - "Medical AI applications"
  - "Artificial intelligence healthcare use cases"
  - "AI technologies in clinical settings"

## ğŸ“Š Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `chromadb` | >=0.4.0 | Vector database |
| `sentence-transformers` | >=2.0.0 | Text embeddings |
| `google-generativeai` | >=0.3.0 | Gemini API client |
| `python-dotenv` | >=1.0.0 | Environment variable management |
| `numpy` | >=1.21.0 | Numerical computations |
| `torch` | >=1.9.0 | PyTorch backend |

## ğŸ› ï¸ Customization

### Configuration

Edit `config.py` to customize:

- **Model Selection**: Change `GEMINI_MODEL` (default: `gemini-2.5-flash`)
- **Agent Parameters**: Adjust temperature, max_tokens, etc.
- **Retrieval Settings**: Number of chunks per query
- **Evaluation Thresholds**: Answer quality scoring criteria
- **Chunking**: Word limits and overlap settings

### Adding Your Own Documents

1. Place your `.txt` files in the `docs/` directory
2. Run the system - it will automatically process new documents
3. Documents are chunked with 20% overlap for better context preservation

### Modifying Chunk Size

Edit `config.py`:
```python
CHUNK_CONFIG = {
    "max_words": 100,      # Chunk size
    "overlap_words": 20,   # Overlap between chunks
}
```

## ğŸ” How It Works

### Routing Flow

1. **Query Reception**: User submits query
2. **Basic Generation**: Router sends query to Basic Generator
   - Simple vector retrieval (top 3 chunks)
   - Straightforward LLM generation
3. **Quality Evaluation**: Router evaluates answer completeness and confidence
4. **Advanced Generation** (if needed):
   - Query Decomposition: Break into sub-queries
   - HyDE: Generate hypothetical answer and search
   - Multi-Query: Generate query variations
   - Combine results and synthesize final answer
5. **Response**: Return final answer with metadata

### Answer Evaluation

The Router Agent uses LLM-based evaluation to assess:
- **Completeness**: Does the answer fully address the question?
- **Relevance**: Is the answer relevant to the query?
- **Confidence**: How confident is the system in the answer?

## ğŸ¯ Performance

- **Response Time**: ~3-8 seconds (basic) or ~10-20 seconds (advanced)
- **Memory Usage**: ~2-3GB (embeddings + models)
- **Storage**: ~500MB for embeddings and documents
- **Accuracy**: High relevance, especially for complex multi-part queries

## ğŸ”„ Workflow Example

```
Query: "Compare AI in healthcare and finance"

[Router] â†’ Using Basic Generator Agent
[Basic] Retrieved 3 chunks
[Router] Evaluating answer... Insufficient
[Router] â†’ Using Advanced Generator Agent

[Advanced/Decomposition] Generated 3 sub-queries
[Advanced/HyDE] Generated hypothetical answer
[Advanced/Multi-Query] Generated 4 query variations
[Advanced] Combined 12 unique chunks
[Advanced] Final answer generated

Answer: [Comprehensive comparison with specific examples]
```

## ğŸš§ Future Enhancements

- [ ] Web interface with Gradio/Streamlit
- [ ] Support for PDF and Word documents
- [ ] Additional retrieval techniques (RAG-Fusion, etc.)
- [ ] Conversation memory and context management
- [ ] Fine-tuned evaluation models
- [ ] Multi-modal support (images, tables)
- [ ] Streaming responses
- [ ] Batch processing for multiple queries

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).



*Built with â¤ï¸ for intelligent, adaptive AI applications*
